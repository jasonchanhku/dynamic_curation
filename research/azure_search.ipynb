{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports will be needed to interact with Azure Blob Storage and Azure Search.\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# import ast\n",
    "# import html\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from openai import AzureOpenAI\n",
    "import re\n",
    "import tempfile\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from functools import partial\n",
    "from typing import Callable, List, Dict, Optional, Generator, Tuple, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "import shutil\n",
    "\n",
    "import markdown\n",
    "import tiktoken\n",
    "# from azure.identity import DefaultAzureCredential\n",
    "# from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import TextSplitter, MarkdownTextSplitter, RecursiveCharacterTextSplitter, PythonCodeTextSplitter\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "FILE_FORMAT_DICT = {\n",
    "    \"md\": \"markdown\",\n",
    "    \"txt\": \"text\",\n",
    "    \"html\": \"html\",\n",
    "    \"shtml\": \"html\",\n",
    "    \"htm\": \"html\",\n",
    "    \"py\": \"python\",\n",
    "    \"pdf\": \"pdf\",\n",
    "    \"docx\": \"docx\",\n",
    "    \"pptx\": \"pptx\"\n",
    "}\n",
    "\n",
    "RETRY_COUNT = 5\n",
    "\n",
    "SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
    "WORDS_BREAKS = list(reversed([\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \"\\n\"]))\n",
    "\n",
    "HTML_TABLE_TAGS = {\"table_open\": \"<table>\", \"table_close\": \"</table>\", \"row_open\":\"<tr>\"}\n",
    "\n",
    "PDF_HEADERS = {\n",
    "    \"title\": \"h1\",\n",
    "    \"sectionHeading\": \"h2\"\n",
    "}\n",
    "\n",
    "class TokenEstimator(object):\n",
    "    GPT2_TOKENIZER = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    def estimate_tokens(self, text: Union[str, List]) -> int:\n",
    "\n",
    "        return len(self.GPT2_TOKENIZER.encode(text, allowed_special=\"all\"))\n",
    "\n",
    "    def construct_tokens_with_size(self, tokens: str, numofTokens: int) -> str:\n",
    "        newTokens = self.GPT2_TOKENIZER.decode(\n",
    "            self.GPT2_TOKENIZER.encode(tokens, allowed_special=\"all\")[:numofTokens]\n",
    "        )\n",
    "        return newTokens\n",
    "\n",
    "TOKEN_ESTIMATOR = TokenEstimator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadBlobUrlToLocalFolder(blob_name, destination_folder):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(os.getenv('AZURE_BLOB_STORAGE_CS'))\n",
    "    container_client = blob_service_client.get_container_client(os.getenv(\"AZURE_BLOB_STORAGE_CONTAINER_NAME\"))\n",
    "    last_destination_folder = None\n",
    "    if destination_folder != last_destination_folder:\n",
    "        os.makedirs(destination_folder, exist_ok=True)\n",
    "        last_destination_folder = destination_folder\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "    with open(file=os.path.join(destination_folder, blob_name), mode='wb') as local_file:\n",
    "        stream = blob_client.download_blob()\n",
    "        local_file.write(stream.readall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_escaped_to_posix(escaped_path):\n",
    "    windows_path = escaped_path.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    posix_path = windows_path.replace(\"\\\\\", \"/\")\n",
    "    return posix_path\n",
    "\n",
    "def _get_file_format(file_name: str, extensions_to_process: List[str]) -> Optional[str]:\n",
    "    \"\"\"Gets the file format from the file name.\n",
    "    Returns None if the file format is not supported.\n",
    "    Args:\n",
    "        file_name (str): The file name.\n",
    "        extensions_to_process (List[str]): List of extensions to process.\n",
    "    Returns:\n",
    "        str: The file format.\n",
    "    \"\"\"\n",
    "\n",
    "    # in case the caller gives us a file path\n",
    "    file_name = os.path.basename(file_name)\n",
    "    file_extension = file_name.split(\".\")[-1]\n",
    "    if file_extension not in extensions_to_process:\n",
    "        return None\n",
    "    return FILE_FORMAT_DICT.get(file_extension, None)\n",
    "\n",
    "def cleanup_content(content: str) -> str:\n",
    "    \"\"\"Cleans up the given content using regexes\n",
    "    Args:\n",
    "        content (str): The content to clean up.\n",
    "    Returns:\n",
    "        str: The cleaned up content.\n",
    "    \"\"\"\n",
    "    output = re.sub(r\"\\n{2,}\", \"\\n\", content)\n",
    "    output = re.sub(r\"[^\\S\\n]{2,}\", \" \", output)\n",
    "    output = re.sub(r\"-{2,}\", \"--\", output)\n",
    "\n",
    "    return output.strip()\n",
    "\n",
    "@dataclass\n",
    "class Document(object):\n",
    "    \"\"\"A data class for storing documents\n",
    "\n",
    "    Attributes:\n",
    "        content (str): The content of the document.\n",
    "        id (Optional[str]): The id of the document.\n",
    "        title (Optional[str]): The title of the document.\n",
    "        filepath (Optional[str]): The filepath of the document.\n",
    "        url (Optional[str]): The url of the document.\n",
    "        metadata (Optional[Dict]): The metadata of the document.    \n",
    "    \"\"\"\n",
    "\n",
    "    content: str\n",
    "    id: Optional[str] = None\n",
    "    title: Optional[str] = None\n",
    "    filepath: Optional[str] = None\n",
    "    url: Optional[str] = None\n",
    "    metadata: Optional[Dict] = None\n",
    "    contentVector: Optional[List[float]] = None\n",
    "    \n",
    "@dataclass\n",
    "class ChunkingResult:\n",
    "    \"\"\"Data model for chunking result\n",
    "\n",
    "    Attributes:\n",
    "        chunks (List[Document]): List of chunks.\n",
    "        total_files (int): Total number of files.\n",
    "        num_unsupported_format_files (int): Number of files with unsupported format.\n",
    "        num_files_with_errors (int): Number of files with errors.\n",
    "        skipped_chunks (int): Number of chunks skipped.\n",
    "    \"\"\"\n",
    "    chunks: List[Document]\n",
    "    total_files: int\n",
    "    num_unsupported_format_files: int = 0\n",
    "    num_files_with_errors: int = 0\n",
    "    # some chunks might be skipped to small number of tokens\n",
    "    skipped_chunks: int = 0\n",
    "\n",
    "\n",
    "class BaseParser(ABC):\n",
    "    \"\"\"A parser parses content to produce a document.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse(self, content: str, file_name: Optional[str] = None) -> Document:\n",
    "        \"\"\"Parses the given content.\n",
    "        Args:\n",
    "            content (str): The content to parse.\n",
    "            file_name (str): The file name associated with the content.\n",
    "        Returns:\n",
    "            Document: The parsed document.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def parse_file(self, file_path: str) -> Document:\n",
    "        \"\"\"Parses the given file.\n",
    "        Args:\n",
    "            file_path (str): The file to parse.\n",
    "        Returns:\n",
    "            Document: The parsed document.\n",
    "        \"\"\"\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return self.parse(f.read(), os.path.basename(file_path))\n",
    "\n",
    "    def parse_directory(self, directory_path: str) -> List[Document]:\n",
    "        \"\"\"Parses the given directory.\n",
    "        Args:\n",
    "            directory_path (str): The directory to parse.\n",
    "        Returns:\n",
    "            List[Document]: List of parsed documents.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        for file_name in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                documents.append(self.parse_file(file_path))\n",
    "        return documents\n",
    "\n",
    "\n",
    "class HTMLParser(BaseParser):\n",
    "    \"\"\"Parses HTML content.\"\"\"\n",
    "    TITLE_MAX_TOKENS = 128\n",
    "    NEWLINE_TEMPL = \"<NEWLINE_TEXT>\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.token_estimator = TokenEstimator()\n",
    "\n",
    "    def parse(self, content: str, file_name: Optional[str] = None) -> Document:\n",
    "        \"\"\"Parses the given content.\n",
    "        Args:\n",
    "            content (str): The content to parse.\n",
    "            file_name (str): The file name associated with the content.\n",
    "        Returns:\n",
    "            Document: The parsed document.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Extract the title\n",
    "        title = ''\n",
    "        if soup.title and soup.title.string:\n",
    "            title = soup.title.string\n",
    "        else:\n",
    "            # Try to find the first <h1> tag\n",
    "            h1_tag = soup.find('h1')\n",
    "            if h1_tag:\n",
    "                title = h1_tag.get_text(strip=True)\n",
    "            else:\n",
    "                h2_tag = soup.find('h2')\n",
    "                if h2_tag:\n",
    "                    title = h2_tag.get_text(strip=True)\n",
    "        if title is None or title == '':\n",
    "            # if title is still not found, guess using the next string\n",
    "            try:\n",
    "                title = next(soup.stripped_strings)\n",
    "                title = self.token_estimator.construct_tokens_with_size(title, self.TITLE_MAX_TOKENS)\n",
    "\n",
    "            except StopIteration:\n",
    "                title = file_name\n",
    "\n",
    "                # Helper function to process text nodes\n",
    "\n",
    "        # Parse the content as it is without any formatting changes\n",
    "        result = content\n",
    "        if title is None:\n",
    "            title = '' # ensure no 'None' type title\n",
    "\n",
    "        return Document(content=cleanup_content(result), title=str(title))\n",
    "\n",
    "class MarkdownParser(BaseParser):\n",
    "    \"\"\"Parses Markdown content.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._html_parser = HTMLParser()\n",
    "\n",
    "    def parse(self, content: str, file_name: Optional[str] = None) -> Document:\n",
    "        \"\"\"Parses the given content.\n",
    "        Args:\n",
    "            content (str): The content to parse.\n",
    "            file_name (str): The file name associated with the content.\n",
    "        Returns:\n",
    "            Document: The parsed document.\n",
    "        \"\"\"\n",
    "        #html_content = markdown.markdown(content, extensions=['fenced_code', 'toc', 'tables', 'sane_lists'])\n",
    "\n",
    "        return Document(content=content, title=\"\")\n",
    "    \n",
    "\n",
    "class ParserFactory:\n",
    "    def __init__(self):\n",
    "        self._parsers = {\n",
    "            \"markdown\": MarkdownParser(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def supported_formats(self) -> List[str]:\n",
    "        \"Returns a list of supported formats\"\n",
    "        return list(self._parsers.keys())\n",
    "\n",
    "    def __call__(self, file_format: str) -> BaseParser:\n",
    "        parser = self._parsers.get(file_format, None)\n",
    "        if parser is None:\n",
    "            raise ValueError(f\"{file_format} is not supported\")\n",
    "\n",
    "        return parser\n",
    "\n",
    "parser_factory = ParserFactory()\n",
    "\n",
    "def merge_chunks_serially(chunked_content_list: List[str], num_tokens: int, url_dict: Dict[str, str]={}) -> Generator[Tuple[str, int], None, None]:\n",
    "    def unmask_urls(text, url_dict={}):\n",
    "        if \"##URL\" in text:\n",
    "            for key, value in url_dict.items():\n",
    "                text = text.replace(key, value)\n",
    "        return text\n",
    "    # TODO: solve for token overlap\n",
    "    current_chunk = \"\"\n",
    "    total_size = 0\n",
    "    for chunked_content in chunked_content_list:\n",
    "        chunked_content = unmask_urls(chunked_content, url_dict)\n",
    "        chunk_size = TOKEN_ESTIMATOR.estimate_tokens(chunked_content)\n",
    "        if total_size > 0:\n",
    "            new_size = total_size + chunk_size\n",
    "            if new_size > num_tokens:\n",
    "                yield current_chunk, total_size\n",
    "                current_chunk = \"\"\n",
    "                total_size = 0\n",
    "        total_size += chunk_size\n",
    "        current_chunk += chunked_content\n",
    "    if total_size > 0:\n",
    "        yield current_chunk, total_size\n",
    "\n",
    "def chunk_content_helper(\n",
    "        content: str, file_format: str, file_name: Optional[str],\n",
    "        token_overlap: int,\n",
    "        num_tokens: int = 256\n",
    ") -> Generator[Tuple[str, int, Document], None, None]:\n",
    "    if num_tokens is None:\n",
    "        num_tokens = 1000000000\n",
    "\n",
    "    parser = parser_factory(file_format.split(\"_pdf\")[0]) # to handle cracked pdf converted to html\n",
    "    doc = parser.parse(content, file_name=file_name)\n",
    "    # if the original doc after parsing is < num_tokens return as it is\n",
    "    doc_content_size = TOKEN_ESTIMATOR.estimate_tokens(doc.content)\n",
    "    if doc_content_size < num_tokens:\n",
    "        yield doc.content, doc_content_size, doc\n",
    "    else:\n",
    "        if file_format == \"markdown\":\n",
    "            splitter = MarkdownTextSplitter.from_tiktoken_encoder(\n",
    "                chunk_size=num_tokens, chunk_overlap=token_overlap)\n",
    "            chunked_content_list = splitter.split_text(\n",
    "                content)  # chunk the original content\n",
    "            for chunked_content, chunk_size in merge_chunks_serially(chunked_content_list, num_tokens):\n",
    "                chunk_doc = parser.parse(chunked_content, file_name=file_name)\n",
    "                chunk_doc.title = doc.title\n",
    "                yield chunk_doc.content, chunk_size, chunk_doc\n",
    "\n",
    "class UnsupportedFormatError(Exception):\n",
    "    \"\"\"Exception raised when a format is not supported by a parser.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_embedding(text, embedding_model_endpoint=None, embedding_model_key=None, azure_credential=None):\n",
    "    endpoint = embedding_model_endpoint if embedding_model_endpoint else os.getenv(\"EMBEDDING_MODEL_ENDPOINT\")\n",
    "    key = embedding_model_key if embedding_model_key else os.getenv(\"EMBEDDING_MODEL_KEY\")\n",
    "    \n",
    "    if azure_credential is None and (endpoint is None or key is None):\n",
    "        raise Exception(\"EMBEDDING_MODEL_ENDPOINT and EMBEDDING_MODEL_KEY are required for embedding\")\n",
    "\n",
    "    try:\n",
    "        endpoint_parts = endpoint.split(\"/openai/deployments/\")\n",
    "        base_url = endpoint_parts[0]\n",
    "        #print(base_url)\n",
    "        deployment_id = endpoint_parts[1].split(\"/embeddings\")[0]\n",
    "        #print(deployment_id)\n",
    "\n",
    "        api_version = endpoint_parts[1].split(\"api-version=\")[1].split(\"&\")[0]\n",
    "\n",
    "        # if azure_credential is not None:\n",
    "        #     api_key = azure_credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "        # else:\n",
    "        api_key = key\n",
    "\n",
    "        client = AzureOpenAI(api_version=api_version, azure_endpoint=base_url, api_key=api_key)\n",
    "        embeddings = client.embeddings.create(model=deployment_id, input=text)\n",
    "        return embeddings.data[0].embedding\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error getting embeddings with endpoint={endpoint} with error={e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_content(\n",
    "    content: str,\n",
    "    file_name: Optional[str] = None,\n",
    "    url: Optional[str] = None,\n",
    "    ignore_errors: bool = True,\n",
    "    num_tokens: int = 256,\n",
    "    min_chunk_size: int = 10,\n",
    "    token_overlap: int = 0,\n",
    "    extensions_to_process = FILE_FORMAT_DICT.keys(),\n",
    "    cracked_pdf = False,\n",
    "    use_layout = False,\n",
    "    add_embeddings = False,\n",
    "    azure_credential = None,\n",
    "    embedding_endpoint = None\n",
    ") -> ChunkingResult:\n",
    "    \"\"\"Chunks the given content. If ignore_errors is true, returns None\n",
    "        in case of an error\n",
    "    Args:\n",
    "        content (str): The content to chunk.\n",
    "        file_name (str): The file name. used for title, file format detection.\n",
    "        url (str): The url. used for title.\n",
    "        ignore_errors (bool): If true, ignores errors and returns None.\n",
    "        num_tokens (int): The number of tokens in each chunk.\n",
    "        min_chunk_size (int): The minimum chunk size below which chunks will be filtered.\n",
    "        token_overlap (int): The number of tokens to overlap between chunks.\n",
    "    Returns:\n",
    "        List[Document]: List of chunked documents.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if file_name is None or (cracked_pdf and not use_layout):\n",
    "            file_format = \"text\"\n",
    "        elif cracked_pdf:\n",
    "            file_format = \"html_pdf\" # differentiate it from native html\n",
    "        else:\n",
    "            file_format = _get_file_format(file_name, extensions_to_process)\n",
    "            if file_format is None:\n",
    "                raise Exception(\n",
    "                    f\"{file_name} is not supported\")\n",
    "\n",
    "        chunked_context = chunk_content_helper(\n",
    "            content=content,\n",
    "            file_name=file_name,\n",
    "            file_format=file_format,\n",
    "            num_tokens=num_tokens,\n",
    "            token_overlap=token_overlap\n",
    "        )\n",
    "        chunks = []\n",
    "        skipped_chunks = 0\n",
    "        for chunk, chunk_size, doc in chunked_context:\n",
    "            if chunk_size >= min_chunk_size:\n",
    "                if add_embeddings:\n",
    "                    for _ in range(RETRY_COUNT):\n",
    "                        try:\n",
    "                            doc.contentVector = get_embedding(chunk, azure_credential=azure_credential, embedding_model_endpoint=embedding_endpoint)\n",
    "                            break\n",
    "                        except:\n",
    "                            time.sleep(30)\n",
    "                    if doc.contentVector is None:\n",
    "                        raise Exception(f\"Error getting embedding for chunk={chunk}\")\n",
    "                    \n",
    "\n",
    "                chunks.append(\n",
    "                    Document(\n",
    "                        content=chunk,\n",
    "                        title=doc.title,\n",
    "                        url=url,\n",
    "                        contentVector=doc.contentVector\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                skipped_chunks += 1\n",
    "\n",
    "    except UnsupportedFormatError as e:\n",
    "        if ignore_errors:\n",
    "            return ChunkingResult(\n",
    "                chunks=[], total_files=1, num_unsupported_format_files=1\n",
    "            )\n",
    "        else:\n",
    "            raise e\n",
    "    except Exception as e:\n",
    "        if ignore_errors:\n",
    "            return ChunkingResult(chunks=[], total_files=1, num_files_with_errors=1)\n",
    "        else:\n",
    "            raise e\n",
    "    return ChunkingResult(\n",
    "        chunks=chunks,\n",
    "        total_files=1,\n",
    "        skipped_chunks=skipped_chunks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(\n",
    "    file_path: str,\n",
    "    ignore_errors: bool = True,\n",
    "    num_tokens=256,\n",
    "    min_chunk_size=10,\n",
    "    url = None,\n",
    "    token_overlap: int = 0,\n",
    "    extensions_to_process = FILE_FORMAT_DICT.keys(),\n",
    "    form_recognizer_client = None,\n",
    "    use_layout = False,\n",
    "    add_embeddings=False,\n",
    "    azure_credential = None,\n",
    "    embedding_endpoint = None\n",
    ") -> ChunkingResult:\n",
    "    \"\"\"Chunks the given file.\n",
    "    Args:\n",
    "        file_path (str): The file to chunk.\n",
    "    Returns:\n",
    "        List[Document]: List of chunked documents.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_format = _get_file_format(file_name, extensions_to_process)\n",
    "    if not file_format:\n",
    "        if ignore_errors:\n",
    "            return ChunkingResult(\n",
    "                chunks=[], total_files=1, num_unsupported_format_files=1\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"{file_name} is not supported\")\n",
    "\n",
    "    cracked_pdf = False\n",
    "    if file_format in [\"pdf\", \"docx\", \"pptx\"]:\n",
    "        print(\"not supported\")\n",
    "    else:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "                content = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            from chardet import detect\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                binary_content = f.read()\n",
    "                encoding = detect(binary_content).get('encoding', 'utf8')\n",
    "                content = binary_content.decode(encoding)\n",
    "        \n",
    "    return chunk_content(\n",
    "        content=content,\n",
    "        file_name=file_name,\n",
    "        ignore_errors=ignore_errors,\n",
    "        num_tokens=num_tokens,\n",
    "        min_chunk_size=min_chunk_size,\n",
    "        url=url,\n",
    "        token_overlap=max(0, token_overlap),\n",
    "        extensions_to_process=extensions_to_process,\n",
    "        cracked_pdf=cracked_pdf,\n",
    "        use_layout=use_layout,\n",
    "        add_embeddings=add_embeddings,\n",
    "        azure_credential=azure_credential,\n",
    "        embedding_endpoint=embedding_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(\n",
    "        file_path: str, # !IMP: Please keep this as the first argument\n",
    "        directory_path: str,\n",
    "        ignore_errors: bool = True,\n",
    "        num_tokens: int = 1024,\n",
    "        min_chunk_size: int = 10,\n",
    "        url_prefix = None,\n",
    "        token_overlap: int = 0,\n",
    "        extensions_to_process: List[str] = FILE_FORMAT_DICT.keys(),\n",
    "        form_recognizer_client = None,\n",
    "        use_layout = False,\n",
    "        add_embeddings = False,\n",
    "        azure_credential = None,\n",
    "        embedding_endpoint = None\n",
    "    ):\n",
    "\n",
    "    if not form_recognizer_client:\n",
    "        form_recognizer_client = None\n",
    "\n",
    "    is_error = False\n",
    "    try:\n",
    "        url_path = None\n",
    "        rel_file_path = os.path.relpath(file_path, directory_path)\n",
    "        if url_prefix:\n",
    "            url_path = url_prefix + rel_file_path\n",
    "            url_path = convert_escaped_to_posix(url_path)\n",
    "\n",
    "        result = chunk_file(\n",
    "            file_path,\n",
    "            ignore_errors=ignore_errors,\n",
    "            num_tokens=num_tokens,\n",
    "            min_chunk_size=min_chunk_size,\n",
    "            url=url_path,\n",
    "            token_overlap=token_overlap,\n",
    "            extensions_to_process=extensions_to_process,\n",
    "            form_recognizer_client=form_recognizer_client,\n",
    "            use_layout=use_layout,\n",
    "            add_embeddings=add_embeddings,\n",
    "            azure_credential=azure_credential,\n",
    "            embedding_endpoint=embedding_endpoint\n",
    "        )\n",
    "        for chunk_idx, chunk_doc in enumerate(result.chunks):\n",
    "            chunk_doc.filepath = rel_file_path\n",
    "            chunk_doc.metadata = json.dumps({\"chunk_id\": str(chunk_idx)})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if not ignore_errors:\n",
    "            raise\n",
    "        print(f\"File ({file_path}) failed with \", e)\n",
    "        is_error = True\n",
    "        result =None\n",
    "    return result, is_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_directory(\n",
    "        file_path: str,\n",
    "        ignore_errors: bool = False,\n",
    "        num_tokens= None, #int = 1024,\n",
    "        min_chunk_size: int = 10,\n",
    "        url_prefix = None,\n",
    "        token_overlap: int = 0,\n",
    "        extensions_to_process: List[str] = list(FILE_FORMAT_DICT.keys()),\n",
    "        form_recognizer_client = None,\n",
    "        use_layout = False,\n",
    "        add_embeddings = True,\n",
    "        azure_credential = None,\n",
    "        embedding_endpoint = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Chunks the given directory recursively\n",
    "    Args:\n",
    "        directory_path (str): The directory to chunk.\n",
    "        ignore_errors (bool): If true, ignores errors and returns None.\n",
    "        num_tokens (int): The number of tokens to use for chunking.\n",
    "        min_chunk_size (int): The minimum chunk size.\n",
    "        url_prefix (str): The url prefix to use for the files. If None, the url will be None. If not None, the url will be url_prefix + relpath. \n",
    "                            For example, if the directory path is /home/user/data and the url_prefix is https://example.com/data, \n",
    "                            then the url for the file /home/user/data/file1.txt will be https://example.com/data/file1.txt\n",
    "        token_overlap (int): The number of tokens to overlap between chunks.\n",
    "        extensions_to_process (List[str]): The list of extensions to process. \n",
    "        form_recognizer_client: Optional form recognizer client to use for pdf files.\n",
    "        use_layout (bool): If true, uses Layout model for pdf files. Otherwise, uses Read.\n",
    "        add_embeddings (bool): If true, adds a vector embedding to each chunk using the embedding model endpoint and key.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of chunked documents.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    total_files = 0\n",
    "    num_unsupported_format_files = 0\n",
    "    num_files_with_errors = 0\n",
    "    skipped_chunks = 0\n",
    "\n",
    "    # all_files_directory = get_files_recursively(directory_path)\n",
    "    #files_to_process = [file_path for file_path in all_files_directory if os.path.isfile(file_path)]\n",
    "    #print(f\"Total files to process={len(files_to_process)} out of total directory size={len(all_files_directory)}\")\n",
    "\n",
    "\n",
    "    print(\"Single process to chunk and parse the files. --njobs > 1 can help performance.\")\n",
    "    #for file_path in tqdm(files_to_process):\n",
    "    directory_path = os.path.dirname(file_path)\n",
    "    total_files += 1\n",
    "    result, is_error = process_file(file_path=file_path,directory_path=directory_path, ignore_errors=ignore_errors,\n",
    "                                num_tokens=num_tokens,\n",
    "                                min_chunk_size=min_chunk_size, url_prefix=url_prefix,\n",
    "                                token_overlap=token_overlap,\n",
    "                                extensions_to_process=extensions_to_process,\n",
    "                                form_recognizer_client=form_recognizer_client, use_layout=use_layout, add_embeddings=add_embeddings,\n",
    "                                azure_credential=azure_credential, embedding_endpoint=embedding_endpoint)\n",
    "    if is_error:\n",
    "        num_files_with_errors += 1\n",
    "        \n",
    "    chunks.extend(result.chunks)\n",
    "    num_unsupported_format_files += result.num_unsupported_format_files\n",
    "    num_files_with_errors += result.num_files_with_errors\n",
    "    skipped_chunks += result.skipped_chunks\n",
    "\n",
    "    return ChunkingResult(\n",
    "            chunks=chunks,\n",
    "            total_files=total_files,\n",
    "            num_unsupported_format_files=num_unsupported_format_files,\n",
    "            num_files_with_errors=num_files_with_errors,\n",
    "            skipped_chunks=skipped_chunks,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_documents_to_index(blob_page, service_name, index_name, docs, credential=None, upload_batch_size = 50, admin_key=None):\n",
    "    if credential is None and admin_key is None:\n",
    "        raise ValueError(\"credential and admin_key cannot be None\")\n",
    "    \n",
    "    to_upload_dicts = []\n",
    "\n",
    "    id = int(blob_page.split(\"page_\")[-1].split(\".\")[0])\n",
    "    \n",
    "    for d in docs:\n",
    "        if type(d) is not dict:\n",
    "            d = asdict(d)\n",
    "        # add id to documents\n",
    "        d.update({\"@search.action\": \"upload\", \"id\": str(id)})\n",
    "        if \"contentVector\" in d and d[\"contentVector\"] is None:\n",
    "            del d[\"contentVector\"]\n",
    "        to_upload_dicts.append(d)\n",
    "        id += 1\n",
    "    \n",
    "    endpoint = \"https://{}.search.windows.net/\".format(service_name)\n",
    "\n",
    "    search_client = SearchClient(\n",
    "        endpoint=endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=AzureKeyCredential(admin_key),\n",
    "    )\n",
    "    # Upload the documents in batches of upload_batch_size\n",
    "    for i in tqdm(range(0, len(to_upload_dicts), upload_batch_size), desc=\"Indexing Chunks...\"):\n",
    "        batch = to_upload_dicts[i: i + upload_batch_size]\n",
    "        results = search_client.upload_documents(documents=batch)\n",
    "        num_failures = 0\n",
    "        errors = set()\n",
    "        for result in results:\n",
    "            if not result.succeeded:\n",
    "                print(f\"Indexing Failed for {result.key} with ERROR: {result.error_message}\")\n",
    "                num_failures += 1\n",
    "                errors.add(result.error_message)\n",
    "        if num_failures > 0:\n",
    "            raise Exception(f\"INDEXING FAILED for {num_failures} documents. Please recreate the index.\"\n",
    "                            f\"To Debug: PLEASE CHECK chunk_size and upload_batch_size. \\n Error Messages: {list(errors)}\")\n",
    "        \n",
    "\n",
    "def create_or_update_search_index(\n",
    "        service_name, \n",
    "        index_name=\"dynamic-idx\", \n",
    "        semantic_config_name=\"default\", \n",
    "        credential=None, \n",
    "        language=\"en\",\n",
    "        vector_config_name=\"default\",\n",
    "        admin_key=None):\n",
    "    \n",
    "    if credential is None and admin_key is None:\n",
    "        raise ValueError(\"credential and admin key cannot be None\")\n",
    "\n",
    "    url = f\"https://{service_name}.search.windows.net/indexes/{index_name}?api-version=2023-07-01-Preview\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": admin_key,\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"id\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"searchable\": True,\n",
    "                \"key\": True,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"content\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"searchable\": True,\n",
    "                \"sortable\": False,\n",
    "                \"facetable\": False,\n",
    "                \"filterable\": False,\n",
    "                \"analyzer\": f\"{language}.lucene\" if language else None,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"title\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"searchable\": True,\n",
    "                \"sortable\": False,\n",
    "                \"facetable\": False,\n",
    "                \"filterable\": False,\n",
    "                \"analyzer\": f\"{language}.lucene\" if language else None,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"filepath\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"searchable\": True,\n",
    "                \"sortable\": False,\n",
    "                \"facetable\": False,\n",
    "                \"filterable\": True,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"url\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"searchable\": True,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"metadata\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"searchable\": True,\n",
    "            },\n",
    "        ],\n",
    "        \"suggesters\": [],\n",
    "        \"scoringProfiles\": [],\n",
    "        \"semantic\": {\n",
    "            \"configurations\": [\n",
    "                {\n",
    "                    \"name\": semantic_config_name,\n",
    "                    \"prioritizedFields\": {\n",
    "                        \"titleField\": {\"fieldName\": \"title\"},\n",
    "                        \"prioritizedContentFields\": [{\"fieldName\": \"content\"}],\n",
    "                        \"prioritizedKeywordsFields\": [],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if vector_config_name:\n",
    "        body[\"fields\"].append({\n",
    "            \"name\": \"contentVector\",\n",
    "            \"type\": \"Collection(Edm.Single)\",\n",
    "            \"searchable\": True,\n",
    "            \"retrievable\": True,\n",
    "            \"dimensions\": 1536,\n",
    "            \"vectorSearchConfiguration\": vector_config_name\n",
    "        })\n",
    "\n",
    "        body[\"vectorSearch\"] = {\n",
    "            \"algorithmConfigurations\": [\n",
    "                {\n",
    "                    \"name\": vector_config_name,\n",
    "                    \"kind\": \"hnsw\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    response = requests.put(url, json=body, headers=headers)\n",
    "    if response.status_code == 201:\n",
    "        print(f\"Created search index {index_name}\")\n",
    "    elif response.status_code == 204:\n",
    "        print(f\"Updated existing search index {index_name}\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to create search index. Error: {response.text}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_content(path: Optional[str] = None, service_name=os.getenv(\"AZURE_SEARCH_SERVICE_NAME\"), index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"), admin_key=os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")):\n",
    "\n",
    "    endpoint = \"https://{}.search.windows.net/\".format(service_name)\n",
    "\n",
    "    search_client = SearchClient(\n",
    "        endpoint=endpoint,\n",
    "        index_name=index_name,\n",
    "        credential=AzureKeyCredential(admin_key),\n",
    "    )\n",
    "    print(f\"Removing sections from '{path or '<all>'}' from search index '{index_name}'\")\n",
    "    filter = None if path is None else f\"filepath eq '{os.path.basename(path)}'\"\n",
    "    result = search_client.search(\"\", filter=filter, top=1000, include_total_count=True)\n",
    "    removed_docs = search_client.delete_documents(\n",
    "        documents=[{\"id\": document[\"id\"]} for document in result]\n",
    "    )\n",
    "    print(f\"\\tRemoved {len(removed_docs)} sections from index\")\n",
    "    # It can take a few seconds for search results to reflect changes, so wait a bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_or_update_search_index(\n",
    "#         \"eaom-manual-search\", \n",
    "#         index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"), \n",
    "#         semantic_config_name=\"default\", \n",
    "#         credential=None, \n",
    "#         language=\"en\",\n",
    "#         vector_config_name=\"default\",\n",
    "#         admin_key=os.getenv(\"AZURE_SEARCH_ADMIN_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_str = os.getenv('AZURE_BLOB_STORAGE_CS')\n",
    "\n",
    "# Create the BlobServiceClient object\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "container_client = blob_service_client.get_container_client(\"dynamic\")\n",
    "blob_list = [blob.name for blob in container_client.list_blobs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading page_010.md\n",
      "downloaded\n",
      "Single process to chunk and parse the files. --njobs > 1 can help performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Chunks...: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed page_010.md\n",
      "downloading page_011.md\n",
      "downloaded\n",
      "Single process to chunk and parse the files. --njobs > 1 can help performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Chunks...: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed page_011.md\n",
      "downloading page_089.md\n",
      "downloaded\n",
      "Single process to chunk and parse the files. --njobs > 1 can help performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Chunks...: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed page_089.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as local_data_folder:\n",
    "    for blob_page in blob_list:\n",
    "        print(f\"downloading {blob_page}\")\n",
    "        downloadBlobUrlToLocalFolder(blob_page, \".\" + local_data_folder)\n",
    "        print(\"downloaded\")\n",
    "        file_path = os.path.join((\".\" + local_data_folder),blob_page)\n",
    "\n",
    "        result = chunk_directory(\n",
    "            file_path, num_tokens=None\n",
    "        )\n",
    "\n",
    "        upload_documents_to_index(blob_page=blob_page, service_name=os.getenv(\"AZURE_SEARCH_SERVICE_NAME\"), index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"), docs=result.chunks, admin_key=os.getenv(\"AZURE_SEARCH_ADMIN_KEY\"))\n",
    "        print(f\"completed {blob_page}\")\n",
    "        shutil.rmtree(os.path.dirname(file_path), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing sections from 'page_089.md' from search index 'dynamic-idx'\n",
      "\tRemoved 1 sections from index\n"
     ]
    }
   ],
   "source": [
    "remove_content(path=\"page_089.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_search_index(blob_page):\n",
    "    with tempfile.TemporaryDirectory() as local_data_folder:\n",
    "        downloadBlobUrlToLocalFolder(blob_page, \".\" + local_data_folder)\n",
    "        print(\"downloaded\")\n",
    "        file_path = os.path.join((\".\" + local_data_folder),blob_page)\n",
    "\n",
    "        result = chunk_directory(\n",
    "            file_path, num_tokens=None\n",
    "        )\n",
    "\n",
    "        upload_documents_to_index(blob_page=blob_page, service_name=os.getenv(\"AZURE_SEARCH_SERVICE_NAME\"), index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"), docs=result.chunks, admin_key=os.getenv(\"AZURE_SEARCH_ADMIN_KEY\"))\n",
    "        print(f\"completed {blob_page}\")\n",
    "        shutil.rmtree(os.path.dirname(file_path), ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
